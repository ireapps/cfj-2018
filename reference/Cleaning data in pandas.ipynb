{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data in pandas\n",
    "\n",
    "For cleaning jobs of any size, specialized tools like [OpenRefine](http://openrefine.org/) are still your best bet -- a typical workflow is to clean your data in OpenRefine, export as a CSV, then load into pandas.\n",
    "\n",
    "But in many cases, you can use some of pandas' built-in tools to whip your data into shape. This is especially useful for data processing tasks that you plan to repeat as the data are updated.\n",
    "\n",
    "Let's import pandas, then we'll run through some scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How dirty is your data?\n",
    "\n",
    "In Excel, running a pivot table (with counts) for each column will show you misspellings, external white space, inconsistent casing and other problems that keep your data from grouping correctly.\n",
    "\n",
    "In SQL, you might do the same thing with The Golden Query™️:\n",
    "\n",
    "```sql\n",
    "SELECT column, COUNT(*)\n",
    "FROM table\n",
    "GROUP BY column\n",
    "ORDER BY 2 DESC\n",
    "```\n",
    "\n",
    "To do the equivalent operation in pandas, you can just call the `value_counts()` method on a column. Let's look at some Congressional junkets data as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "junkets = pd.read_csv('../data/congress_junkets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocID</th>\n",
       "      <th>FilerName</th>\n",
       "      <th>MemberName</th>\n",
       "      <th>State</th>\n",
       "      <th>District</th>\n",
       "      <th>Year</th>\n",
       "      <th>Destination</th>\n",
       "      <th>FilingType</th>\n",
       "      <th>DepartureDate</th>\n",
       "      <th>ReturnDate</th>\n",
       "      <th>TravelSponsor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500005076</td>\n",
       "      <td>Bobby Cornett</td>\n",
       "      <td>Franks, Trent</td>\n",
       "      <td>AZ</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>Original</td>\n",
       "      <td>1/7/2011</td>\n",
       "      <td>1/9/2011</td>\n",
       "      <td>Consumer Electronics Association</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500005077</td>\n",
       "      <td>Michael Strittmatter</td>\n",
       "      <td>Franks, Trent</td>\n",
       "      <td>AZ</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>Original</td>\n",
       "      <td>1/7/2011</td>\n",
       "      <td>1/9/2011</td>\n",
       "      <td>CEA Leaders in Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500005081</td>\n",
       "      <td>Diane Rinaldo</td>\n",
       "      <td>Rogers, Mike</td>\n",
       "      <td>AL</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>Original</td>\n",
       "      <td>1/6/2011</td>\n",
       "      <td>1/8/2011</td>\n",
       "      <td>Consumer Electronics Association</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500005082</td>\n",
       "      <td>Kenneth DeGraff</td>\n",
       "      <td>Doyle, Michael</td>\n",
       "      <td>PA</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>Original</td>\n",
       "      <td>1/6/2011</td>\n",
       "      <td>1/9/2011</td>\n",
       "      <td>Consumer Electronics Association</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500005083</td>\n",
       "      <td>Michael Ryan Clough</td>\n",
       "      <td>Lofgren, Zoe</td>\n",
       "      <td>CA</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>Original</td>\n",
       "      <td>1/6/2011</td>\n",
       "      <td>1/8/2011</td>\n",
       "      <td>Consumer Electronics Association</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DocID             FilerName      MemberName State  District  Year  \\\n",
       "0  500005076         Bobby Cornett   Franks, Trent    AZ       8.0  2011   \n",
       "1  500005077  Michael Strittmatter   Franks, Trent    AZ       8.0  2011   \n",
       "2  500005081         Diane Rinaldo    Rogers, Mike    AL       3.0  2011   \n",
       "3  500005082       Kenneth DeGraff  Doyle, Michael    PA      14.0  2011   \n",
       "4  500005083   Michael Ryan Clough    Lofgren, Zoe    CA      19.0  2011   \n",
       "\n",
       "     Destination FilingType DepartureDate ReturnDate  \\\n",
       "0  Las Vegas, NV   Original      1/7/2011   1/9/2011   \n",
       "1  Las Vegas, NV   Original      1/7/2011   1/9/2011   \n",
       "2  Las Vegas, NV   Original      1/6/2011   1/8/2011   \n",
       "3  Las Vegas, NV   Original      1/6/2011   1/9/2011   \n",
       "4  Las Vegas, NV   Original      1/6/2011   1/8/2011   \n",
       "\n",
       "                      TravelSponsor  \n",
       "0  Consumer Electronics Association  \n",
       "1         CEA Leaders in Technology  \n",
       "2  Consumer Electronics Association  \n",
       "3  Consumer Electronics Association  \n",
       "4  Consumer Electronics Association  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "junkets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run `value_counts()` on the _Destination_ colummn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Baltimore, MD                     827\n",
       "Hot Springs, VA                   753\n",
       "Tel Aviv, Israel                  651\n",
       "New York, NY                      635\n",
       "Philadelphia, PA                  487\n",
       "Cambridge, MD                     468\n",
       "Jerusalem, Israel                 466\n",
       "Las Vegas, NV                     373\n",
       "Williamsburg, VA                  371\n",
       "Istanbul, Turkey                  365\n",
       "Tiberias, Israel                  240\n",
       "Ankara, Turkey                    222\n",
       "Warrenton, VA                     159\n",
       "Boston, MA                        138\n",
       "Los Angeles, CA                   138\n",
       "Middleburg, VA                    137\n",
       "San Francisco, CA                 136\n",
       "Miami, FL                         119\n",
       "Berlin, Germany                   118\n",
       "Hershey, PA                       108\n",
       "Chicago, IL                       105\n",
       "Atlanta, GA                        93\n",
       "Brussels, Belgium                  89\n",
       "Tokyo, Japan                       87\n",
       "New Orleans, LA                    82\n",
       "Palo Alto, CA                      70\n",
       "Havana, Cuba                       68\n",
       "Richmond, VA                       65\n",
       "London, United Kingdom             61\n",
       "San Diego, CA                      61\n",
       "                                 ... \n",
       "Kutaisi, Georgia                    1\n",
       "Fajardo, Puerto Rico                1\n",
       "San Juan, PR                        1\n",
       "Dallas/Fort Worth, TX               1\n",
       "Hazyview, South Africa              1\n",
       "Sharm El-Sheikh, Egypt              1\n",
       "Hagatna, Guam                       1\n",
       "Rehobeth, DE                        1\n",
       "Tiberas, Israel                     1\n",
       "Melbourne, Australia                1\n",
       "Puerto Jordan, Colombia             1\n",
       "Baku, Republic of Azerbaijan        1\n",
       "L'Aquila, Italy                     1\n",
       "Seoul, Korea                        1\n",
       "Port of Spain, Trinidad and To      1\n",
       "Baltimroe, MD                       1\n",
       "Anomabo, Ghana                      1\n",
       "Porterville, CA                     1\n",
       "Minsk, Belarus                      1\n",
       "Kalispell, MT                       1\n",
       "Kalamazoo, MI                       1\n",
       "Pristina, Kosovo,                   1\n",
       "Helsinki, Finland                   1\n",
       "Berling, Germany                    1\n",
       "Omaha, NB                           1\n",
       "Denpasar, Indonesia                 1\n",
       "Charleston, WV                      1\n",
       "Bogata, Columbia                    1\n",
       "Washington, IA                      1\n",
       "Ann Arbor, MI                       1\n",
       "Name: Destination, Length: 838, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "junkets['Destination'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default sort order is by count descending, but it can also be helpful in finding typos to sort by the name -- the \"index\" of what `value_counts()` returns. To do that, tack on `sort_index()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Abidjan, Cote d'Ivoire              3\n",
       "Abu Dhabi, United Arab Emirate      3\n",
       "Abuja, Nigeria                      4\n",
       "Accra                               1\n",
       "Accra, Ghana                       10\n",
       "Addis Ababa, Ethiopia              43\n",
       "Addis, Ethiopia                     3\n",
       "Adelaide, Australia                 1\n",
       "Aiken, SC                          15\n",
       "Akron, OH                           7\n",
       "Albany, NY                         10\n",
       "Alberta, Canada                    13\n",
       "Albuquerque, NM                     8\n",
       "Algiers, Algeria                   11\n",
       "Allentown, PA                       1\n",
       "Amelia Island, FL                   2\n",
       "Ames, IA                           23\n",
       "Amman, Jordan                      17\n",
       "Amsterdam, Netherlands              2\n",
       "Anaheim, CA                         2\n",
       "Anatalya, Turkey                    1\n",
       "Anchorage, AK                       4\n",
       "Andechs, Germany                    9\n",
       "Ankara, Israel                      1\n",
       "Ankara, Turkey                    222\n",
       "Ankeny, IA                          3\n",
       "Ankey, IA                           1\n",
       "Ann Arbor, MI                       1\n",
       "Annapolis, MD                       3\n",
       "Anomabo, Ghana                      1\n",
       "                                 ... \n",
       "West Lake Village, CA               1\n",
       "West Palm Beach                     1\n",
       "West Palm Beach, FL                 4\n",
       "West Palm Peach, FL                 1\n",
       "West Point, NY                      7\n",
       "Westlake Village, CA                6\n",
       "Wheeling, WV                        1\n",
       "White Hall, MD                      2\n",
       "White Sulphur Springs, WV           4\n",
       "Wichita, KS                         2\n",
       "Wiitenberg, Germany                 1\n",
       "Wililamsburg, VA                    1\n",
       "Williamsburg, VA                  371\n",
       "Willimasburg, VA                    2\n",
       "Wilmington, DE                     11\n",
       "Witchita, KS                        1\n",
       "Wittenberg, Germany                 1\n",
       "Wooster, OH                         1\n",
       "Wroclaw, Poland                     4\n",
       "Wytheville, VA                      2\n",
       "Xi'an, China                        4\n",
       "Yambio, South Sudan                 2\n",
       "Yangon, Myanmar                     1\n",
       "Yaounde, Cameroon                  13\n",
       "Yerevan, Armenia                    1\n",
       "Zagreb, Croatia                     1\n",
       "Zanzibar, Tanzania                  6\n",
       "Zhytomyr, Ukraine                   1\n",
       "Zugdidi, Georgia                    1\n",
       "Zurich, Switzerland                 3\n",
       "Name: Destination, Length: 838, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "junkets['Destination'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and now we start to see some common data problems in our 838 unique destinations -- whitespace, inconsistent values for the same thing (\"Accra\" and \"Accra, Ghana\") -- and can start fixing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing whitespace, casing and other \"string\" problems\n",
    "\n",
    "If part of our analysis hinged on having a pristine \"Destination\" column, then we've got some work ahead of us. First thing I'd do: Strip whitespace and upcase the text.\n",
    "\n",
    "You can do a lot of basic cleanup like this by applying Python's built-in string methods to the `str` attribute of a column.\n",
    "\n",
    "👉 For more information on Python string methods, [check out this notebook](Python%20data%20types%20and%20basic%20syntax.ipynb#String-methods).\n",
    "\n",
    "To start with, let's create a new column, `destination_clean`, with a stripped/uppercase version of the destination data.\n",
    "\n",
    "**Note**: Outside of pandas, you can use \"method chaining\" to apply multiple transformations to a string, like this: `'   My String'.upper().strip()`.\n",
    "\n",
    "When you're chaining string methods on the `str` attribute of a pandas column series, though, it doesn't work like that -- you have to call `str` after each method call. In other words:\n",
    "\n",
    "```python\n",
    "# this will throw an error\n",
    "junkets['destination_clean'] = junkets['Destination'].str.upper().strip()\n",
    "\n",
    "# this will work\n",
    "junkets['destination_clean'] = junkets['Destination'].str.upper().str.strip()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "junkets['destination_clean'] = junkets['Destination'].str.upper().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocID</th>\n",
       "      <th>FilerName</th>\n",
       "      <th>MemberName</th>\n",
       "      <th>State</th>\n",
       "      <th>District</th>\n",
       "      <th>Year</th>\n",
       "      <th>Destination</th>\n",
       "      <th>FilingType</th>\n",
       "      <th>DepartureDate</th>\n",
       "      <th>ReturnDate</th>\n",
       "      <th>TravelSponsor</th>\n",
       "      <th>destination_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500005076</td>\n",
       "      <td>Bobby Cornett</td>\n",
       "      <td>Franks, Trent</td>\n",
       "      <td>AZ</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>Original</td>\n",
       "      <td>1/7/2011</td>\n",
       "      <td>1/9/2011</td>\n",
       "      <td>Consumer Electronics Association</td>\n",
       "      <td>LAS VEGAS, NV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500005077</td>\n",
       "      <td>Michael Strittmatter</td>\n",
       "      <td>Franks, Trent</td>\n",
       "      <td>AZ</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>Original</td>\n",
       "      <td>1/7/2011</td>\n",
       "      <td>1/9/2011</td>\n",
       "      <td>CEA Leaders in Technology</td>\n",
       "      <td>LAS VEGAS, NV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500005081</td>\n",
       "      <td>Diane Rinaldo</td>\n",
       "      <td>Rogers, Mike</td>\n",
       "      <td>AL</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>Original</td>\n",
       "      <td>1/6/2011</td>\n",
       "      <td>1/8/2011</td>\n",
       "      <td>Consumer Electronics Association</td>\n",
       "      <td>LAS VEGAS, NV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500005082</td>\n",
       "      <td>Kenneth DeGraff</td>\n",
       "      <td>Doyle, Michael</td>\n",
       "      <td>PA</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>Original</td>\n",
       "      <td>1/6/2011</td>\n",
       "      <td>1/9/2011</td>\n",
       "      <td>Consumer Electronics Association</td>\n",
       "      <td>LAS VEGAS, NV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500005083</td>\n",
       "      <td>Michael Ryan Clough</td>\n",
       "      <td>Lofgren, Zoe</td>\n",
       "      <td>CA</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>Original</td>\n",
       "      <td>1/6/2011</td>\n",
       "      <td>1/8/2011</td>\n",
       "      <td>Consumer Electronics Association</td>\n",
       "      <td>LAS VEGAS, NV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DocID             FilerName      MemberName State  District  Year  \\\n",
       "0  500005076         Bobby Cornett   Franks, Trent    AZ       8.0  2011   \n",
       "1  500005077  Michael Strittmatter   Franks, Trent    AZ       8.0  2011   \n",
       "2  500005081         Diane Rinaldo    Rogers, Mike    AL       3.0  2011   \n",
       "3  500005082       Kenneth DeGraff  Doyle, Michael    PA      14.0  2011   \n",
       "4  500005083   Michael Ryan Clough    Lofgren, Zoe    CA      19.0  2011   \n",
       "\n",
       "     Destination FilingType DepartureDate ReturnDate  \\\n",
       "0  Las Vegas, NV   Original      1/7/2011   1/9/2011   \n",
       "1  Las Vegas, NV   Original      1/7/2011   1/9/2011   \n",
       "2  Las Vegas, NV   Original      1/6/2011   1/8/2011   \n",
       "3  Las Vegas, NV   Original      1/6/2011   1/9/2011   \n",
       "4  Las Vegas, NV   Original      1/6/2011   1/8/2011   \n",
       "\n",
       "                      TravelSponsor destination_clean  \n",
       "0  Consumer Electronics Association     LAS VEGAS, NV  \n",
       "1         CEA Leaders in Technology     LAS VEGAS, NV  \n",
       "2  Consumer Electronics Association     LAS VEGAS, NV  \n",
       "3  Consumer Electronics Association     LAS VEGAS, NV  \n",
       "4  Consumer Electronics Association     LAS VEGAS, NV  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "junkets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run `value_counts()` again to see if that helped at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ABIDJAN, COTE D'IVOIRE              3\n",
       "ABU DHABI, UNITED ARAB EMIRATE      3\n",
       "ABUJA, NIGERIA                      4\n",
       "ACCRA                               1\n",
       "ACCRA, GHANA                       10\n",
       "ADDIS ABABA, ETHIOPIA              43\n",
       "ADDIS, ETHIOPIA                     3\n",
       "ADELAIDE, AUSTRALIA                 1\n",
       "AIKEN, SC                          15\n",
       "AKRON, OH                           7\n",
       "ALBANY, NY                         10\n",
       "ALBERTA, CANADA                    13\n",
       "ALBUQUERQUE, NM                     8\n",
       "ALGIERS, ALGERIA                   11\n",
       "ALLENTOWN, PA                       1\n",
       "AMELIA ISLAND, FL                   2\n",
       "AMES, IA                           23\n",
       "AMMAN, JORDAN                      17\n",
       "AMSTERDAM, NETHERLANDS              2\n",
       "ANAHEIM, CA                         2\n",
       "ANATALYA, TURKEY                    1\n",
       "ANCHORAGE, AK                       4\n",
       "ANDECHS, GERMANY                    9\n",
       "ANKARA, ISRAEL                      1\n",
       "ANKARA, TURKEY                    222\n",
       "ANKENY, IA                          3\n",
       "ANKEY, IA                           1\n",
       "ANN ARBOR, MI                       1\n",
       "ANNAPOLIS, MD                       3\n",
       "ANOMABO, GHANA                      1\n",
       "                                 ... \n",
       "WEST LAKE VILLAGE, CA               1\n",
       "WEST PALM BEACH                     1\n",
       "WEST PALM BEACH, FL                 4\n",
       "WEST PALM PEACH, FL                 1\n",
       "WEST POINT, NY                      7\n",
       "WESTLAKE VILLAGE, CA                6\n",
       "WHEELING, WV                        1\n",
       "WHITE HALL, MD                      2\n",
       "WHITE SULPHUR SPRINGS, WV           4\n",
       "WICHITA, KS                         2\n",
       "WIITENBERG, GERMANY                 1\n",
       "WILILAMSBURG, VA                    1\n",
       "WILLIAMSBURG, VA                  371\n",
       "WILLIMASBURG, VA                    2\n",
       "WILMINGTON, DE                     11\n",
       "WITCHITA, KS                        1\n",
       "WITTENBERG, GERMANY                 1\n",
       "WOOSTER, OH                         1\n",
       "WROCLAW, POLAND                     4\n",
       "WYTHEVILLE, VA                      2\n",
       "XI'AN, CHINA                        4\n",
       "YAMBIO, SOUTH SUDAN                 2\n",
       "YANGON, MYANMAR                     1\n",
       "YAOUNDE, CAMEROON                  13\n",
       "YEREVAN, ARMENIA                    1\n",
       "ZAGREB, CROATIA                     1\n",
       "ZANZIBAR, TANZANIA                  6\n",
       "ZHYTOMYR, UKRAINE                   1\n",
       "ZUGDIDI, GEORGIA                    1\n",
       "ZURICH, SWITZERLAND                 3\n",
       "Name: destination_clean, Length: 831, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "junkets['destination_clean'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That eliminated a handful of problems. Now comes the tedious work of identifying entries to find and replace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk-replacing values with other values\n",
    "\n",
    "If we were at this point in Excel, we'd scroll through the list of unique names and start making notes of what we need to change. Same story here.\n",
    "\n",
    "Let's loop over a [sorted](https://docs.python.org/3/howto/sorting.html) list of `unique()` destinations and `print()` each one.\n",
    "\n",
    "👉 For a refresher on _for loops_, [see this notebook](Python%20data%20types%20and%20basic%20syntax.ipynb#for-loops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABIDJAN, COTE D'IVOIRE\n",
      "ABU DHABI, UNITED ARAB EMIRATE\n",
      "ABUJA, NIGERIA\n",
      "ACCRA\n",
      "ACCRA, GHANA\n",
      "ADDIS ABABA, ETHIOPIA\n",
      "ADDIS, ETHIOPIA\n",
      "ADELAIDE, AUSTRALIA\n",
      "AIKEN, SC\n",
      "AKRON, OH\n",
      "ALBANY, NY\n",
      "ALBERTA, CANADA\n",
      "ALBUQUERQUE, NM\n",
      "ALGIERS, ALGERIA\n",
      "ALLENTOWN, PA\n",
      "AMELIA ISLAND, FL\n",
      "AMES, IA\n",
      "AMMAN, JORDAN\n",
      "AMSTERDAM, NETHERLANDS\n",
      "ANAHEIM, CA\n",
      "ANATALYA, TURKEY\n",
      "ANCHORAGE, AK\n",
      "ANDECHS, GERMANY\n",
      "ANKARA, ISRAEL\n",
      "ANKARA, TURKEY\n",
      "ANKENY, IA\n",
      "ANKEY, IA\n",
      "ANN ARBOR, MI\n",
      "ANNAPOLIS, MD\n",
      "ANOMABO, GHANA\n",
      "ANTALYA, TURKEY\n",
      "ANTIGUA, GUATEMALA\n",
      "ARAUCA, COLOMBIA\n",
      "ARLINGTON, VA\n",
      "ARUSHA, TANZANIA\n",
      "ASHEVILLE, NC\n",
      "ASPEN, CO\n",
      "ATLANTA, GA\n",
      "ATLANTA, GEORGIA\n",
      "ATLANTA,GA\n",
      "AUGUSTA, GA\n",
      "AUSTIN, TEXAS\n",
      "AUSTIN, TX\n",
      "AVENTURA, FL\n",
      "AVILA BEACH, CA\n",
      "AVOCA, IA\n",
      "AWASSA, ETHIOPIA\n",
      "BAKU, AZERBAIJAN\n",
      "BAKU, AZERBIJAN\n",
      "BAKU, REPUBLIC OF AZERBAIJAN\n",
      "BALI, INDONESIA\n",
      "BALTIMORE, DC\n",
      "BALTIMORE, MD\n",
      "BALTIMROE, MD\n",
      "BANFF, CANADA\n",
      "BANGALORE, INDIA\n",
      "BANJA LUKA, BOSNIA-HERZEGOVINA\n",
      "BARCELONA, SPAIN\n",
      "BARTLESVILLE, OK\n",
      "BATON ROUGE, LA\n",
      "BATTLE CREEK, MI\n",
      "BEARDSTOWN, IL\n",
      "BEDFORD SPRINGS, PA\n",
      "BEDFORD, PA\n",
      "BEIJING, CHINA\n",
      "BEIRA, MOZAMBIQUE\n",
      "BEIRUT, LEBANON\n",
      "BEJING, CHINA\n",
      "BELFAST, NORTHERN IRELAND\n",
      "BELGRADE, SERBIA\n",
      "BENTIU, SOUTH SUDAN\n",
      "BERKELEY, CA\n",
      "BERLIN BERMANY\n",
      "BERLIN, GERMANY\n",
      "BERLING, GERMANY\n",
      "BETHLEHEM\n",
      "BETHLEHEM, PA\n",
      "BETHLEHEM, PALESTINIAN TERRITO\n",
      "BIRMINGHAM, AL\n",
      "BIRMINGHAM, ENGLAND\n",
      "BISHKEK, KYRGYZSTAN\n",
      "BISMARCK, ND\n",
      "BISMARK, ND\n",
      "BLAIRSTOWN, IA\n",
      "BLANTYRE, MALAWI\n",
      "BLOOMINGTON, IL\n",
      "BOCA RATON, FL\n",
      "BOGATA, COLUMBIA\n",
      "BOGOTA, COLOMBIA\n",
      "BOGOTA, COLUMBIA\n",
      "BOLOGNA, ITALY\n",
      "BOONE, IA\n",
      "BOSTON, MA\n",
      "BOTOTA, COLUMBIA\n",
      "BOULDER, CO\n",
      "BRETTON WOODS, NH\n",
      "BRIDGEPORT, CT\n",
      "BRIESEN, GERMANY\n",
      "BROOKLYN, NY\n",
      "BRUGES, BELGIUM\n",
      "BRUNSWICK, GA\n",
      "BRUSSELLS, BELGIUM\n",
      "BRUSSELS, BELGIUM\n",
      "BRUSSELS, BELGUIM\n",
      "BUCHAREST, ROMANIA\n",
      "BUDAPEST, HUNGARY\n",
      "BUDVA, MONTENEGRO\n",
      "BUENAVENTURA, COLOMBIA\n",
      "BUENAVENTURA, COLUMBIA\n",
      "BUENAVENTURE, COLUMBIA\n",
      "BUENOS AIRES, ARGENTINA\n",
      "BUJUMBURA, BURUNDI\n",
      "BUKAVU, DEMOCRATIC REPUBLIC OF\n",
      "BURAS, LA\n",
      "BURBANK, CA\n",
      "BURLINGTON, IA\n",
      "BURLINGTON, VT\n",
      "BUSHKILL, PA\n",
      "CACERES, COLOMBIA\n",
      "CAIRO, EGYPT\n",
      "CALEBRA, NIGERIA\n",
      "CALI, COLOMBIA\n",
      "CALI, COLUMBIA\n",
      "CAMBRIDGE, MA\n",
      "CAMBRIDGE, MD\n",
      "CANAKKALE, TURKEY\n",
      "CANNAKALE, TURKEY\n",
      "CANNAKKALE, TURKEY\n",
      "CANONSBURG, PA\n",
      "CANTON, OH\n",
      "CAPADOCIA, TURKEY\n",
      "CAPE TOWN, SOUTH AFRICA\n",
      "CAPPADOCIA, TURKEY\n",
      "CAPRI, ITALY\n",
      "CAREGENA, COLUMBIA\n",
      "CARTAGENA, COLOMBIA\n",
      "CARTAGENA, COLUMBIA\n",
      "CARTEGENA, COLUMBIA\n",
      "CASABLANCA, MOROCCO\n",
      "CAUCASIA, COLOMBIA\n",
      "CAUX, SWITZERLAND\n",
      "CAYAR, SENEGAL\n",
      "CEDAR KEY, FL\n",
      "CEDAR RAPIDS, IA\n",
      "CESME, TURKEY\n",
      "CHALMETTE, LA\n",
      "CHAMPAIGN, IL\n",
      "CHANTILLY, VA\n",
      "CHARLESTON, SC\n",
      "CHARLESTON, WV\n",
      "CHARLOTTE, NC\n",
      "CHARLOTTE, SC\n",
      "CHARLOTTESVILLE, VA\n",
      "CHATTANOOGA, TN\n",
      "CHAUTAUQUA, NY\n",
      "CHENGDU, CHINA\n",
      "CHERITON, VA\n",
      "CHERLTON, VA\n",
      "CHESAPEAKE BAY, MD\n",
      "CHESAPEAKE, MD\n",
      "CHICAGO, IL\n",
      "CHICHICASTENANGO, GUATEMALA\n",
      "CHIPATA, ZAMBIA\n",
      "CINCINNATI, OH\n",
      "CIUDAD JUAREZ, MEXICO\n",
      "CLEARWATER BEACH, FL\n",
      "CLEVELAND, OH\n",
      "CLEVELAND, OHIO\n",
      "CODY, WY\n",
      "COLOMBO, SRI LANKA\n",
      "COLORADO SPRINGS, CO\n",
      "COLUMBIA, MD\n",
      "COLUMBIA, MO\n",
      "COLUMBUS, OH\n",
      "COPENHAGEN, DENMARK\n",
      "CORPUS CHRISTI, TX\n",
      "COUNCIL BLUFFS, IA\n",
      "CULPEPER, VA\n",
      "CUSCO, PERU\n",
      "CUZCO, PERU\n",
      "DAKAR, SENAGAL\n",
      "DAKAR, SENEGAL\n",
      "DALLAS, TX\n",
      "DALLAS/FORT WORTH, TX\n",
      "DAMASCUS, SYRIA\n",
      "DAR ES SALAAM, TANZANIA\n",
      "DAR-ES-SALAAM, TANZANIA\n",
      "DAVENPORT, IA\n",
      "DAYTONA BEACH, FL\n",
      "DEADHORSE, AK\n",
      "DELHI, INDIA\n",
      "DELRAY BEACH, FL\n",
      "DENPASAR, INDONESIA\n",
      "DENVER, CO\n",
      "DES MOINES, IA\n",
      "DES MOINES, IOWA\n",
      "DESTIN, FL\n",
      "DETROIT, MI\n",
      "DHAKA, BANGLADESH\n",
      "DIGOS CITY, PHILIPPINES\n",
      "DILI, EAST TIMOR\n",
      "DILI, TIMOR-LESTE\n",
      "DINGMANS FERRY, PA\n",
      "DIRE DAWA, ETHIOPIA\n",
      "DOHA, QATAR\n",
      "DOLLO ADO AIRFIELD, ETHIOPIA\n",
      "DRESDEN, GERMANY\n",
      "DUBAI, UAE\n",
      "DUBAI, UNITED ARAB EMIRATES\n",
      "DUBLIN, IRELAND\n",
      "DUBUQUE, IA\n",
      "DURBAN, SOUTH AFRICA\n",
      "DURHAM, NC\n",
      "DURRES, ALBANIA\n",
      "DYERSVILLE, IA\n",
      "EDMONTON, CANADA\n",
      "EL CARMEN, GUATEMALA\n",
      "EL PASO, TX\n",
      "EL PROGRESSO, HONDURAS\n",
      "ELDORET, KENYA\n",
      "ELKTON, MD\n",
      "ELMAU OBERBAYERN, GERMANY\n",
      "ELMAU, GERMANY\n",
      "ENTEBBE, UGANDA\n",
      "ENTEBBE,UGANDA\n",
      "ERBIL, IRAQ\n",
      "FAIRBANKS, AK\n",
      "FAIRBURY, IL\n",
      "FAJARDO, PR\n",
      "FAJARDO, PUERTO RICO\n",
      "FARGO, ND\n",
      "FARLEY, IA\n",
      "FARMINGTON, PA\n",
      "FISHING CREEK, MD\n",
      "FLORENCE, ITALY\n",
      "FLOYD, IA\n",
      "FORT COLLINS, CO\n",
      "FORT DODGE, IA\n",
      "FORT LAUDERDALE, FL\n",
      "FORT LAURDERDALE, FL\n",
      "FORT MCMURRAY, CANADA\n",
      "FORT MYERS, FL\n",
      "FORT WAYNE, IN\n",
      "FORT WORTH, TEXAS\n",
      "FORT WORTH, TX\n",
      "FORT WORTH/DALLAS, TX\n",
      "FRANKFURT, GERMANY\n",
      "FREDERICKSBURG, VA\n",
      "FREETOWN, SIERRA LEONE\n",
      "FRESNO, CA\n",
      "FT LAUDERDALE, FL\n",
      "FT. LAUDERDALE, FL\n",
      "FT. MCMURRAY, CANADA\n",
      "FT. MYERS, FL\n",
      "FT. WORTH, TEXAS\n",
      "FUKUSHIMA, JAPAN\n",
      "GALBRAITH LAKE, AK\n",
      "GALVA, IA\n",
      "GAROUA, CAMEROON\n",
      "GAZIANTEP, TURKEY\n",
      "GAZIATEP, TURKEY\n",
      "GETTYSBURG, PA\n",
      "GIBSON CITY, IL\n",
      "GINOSAR, ISRAEL\n",
      "GITEGA, BURUNDI\n",
      "GOLAN HEIGHTS\n",
      "GOMA, DEMOCRATIC REPUBLIC OF C\n",
      "GOMA, DEMOCRATIC REPUBLIC OF T\n",
      "GOREM, TURKEY\n",
      "GORONGOSA NATIONAL PARK, MOZAM\n",
      "GREAT FALLS, MT\n",
      "GREENSBORO, NC\n",
      "GUADALAJARA, MEXICO\n",
      "GUANGZHOU, CHINA\n",
      "GUANZHOU, CHINA\n",
      "GUATAMALA CITY, GUATAMALA\n",
      "GUATEMALA CITY, GUATEMALA\n",
      "GULFPORT, MS\n",
      "HAGATNA, GUAM\n",
      "HAGOSHRIM, ISRAEL\n",
      "HAITI\n",
      "HAMPTON, VA\n",
      "HANOI, VIETNAM\n",
      "HANOVER, NH\n",
      "HARPER'S FERRY, WV\n",
      "HARPERS FERRY, WV\n",
      "HARRISBURG, PA\n",
      "HARRODSBURG, KY\n",
      "HARTFORD, CONNECTICUT\n",
      "HARTFORD, CT\n",
      "HARWICH, MA\n",
      "HARWOOD, MD\n",
      "HAVANA, CUBA\n",
      "HAVANNA, CUBA\n",
      "HAVERFORD, PA\n",
      "HAWASSA, ETHIOPIA\n",
      "HAZYVIEW, SOUTH AFRICA\n",
      "HELSINKI, FINLAND\n",
      "HERSHEY, PA\n",
      "HILLSDALE, MI\n",
      "HILTON HEAD ISLAND, NC\n",
      "HILTON HEAD ISLAND, SC\n",
      "HILTON HEAD, SC\n",
      "HIROSHIMA, JAPAN\n",
      "HO CHI MINH CITY, VIETNAM\n",
      "HOLLYWOOD, FL\n",
      "HONG KONG, CHINA\n",
      "HORTA, AZORES\n",
      "HOT SPRING, VA\n",
      "HOT SPRINGS, VA\n",
      "HOT SPRINGS,VA\n",
      "HOUMA, LA\n",
      "HOUSTON, TX\n",
      "HUNTSVILLE, AL\n",
      "HUXLEY, IA\n",
      "HYATT CHESAPEAKE BAY, CAMBRIDG\n",
      "HYATT REGENCY CHESAPEAKE BAY,\n",
      "IDAHO FALLS, ID\n",
      "INDIAN WELLS, CA\n",
      "INDIANAPOLIS, IN\n",
      "INGA DAM, DEMOCRATIC REPUBLIC\n",
      "INSTANBUL, TURKEY\n",
      "IQUITOS, PERU\n",
      "ISLAMABAD, PAKISTAN\n",
      "ISLAMORADA, FL\n",
      "ISRAEL\n",
      "ISRAEL, JERUSALEM\n",
      "ISTANBUL, ISRAEL\n",
      "ISTANBUL, TUKEY\n",
      "ISTANBUL, TURKEY\n",
      "ITHACA, NY\n",
      "IWAKUNI, JAPAN\n",
      "IWO JIMA, JAPAN\n",
      "IZMIR, TURKEY\n",
      "JACKSON HOLE, WY\n",
      "JACKSON, MS\n",
      "JACKSON, WY\n",
      "JACKSONVILLE, FL\n",
      "JAGDALPUR, INDIA\n",
      "JAKARTA, INDONESIA\n",
      "JEFFERSON PARISH, LA\n",
      "JERSALEM, ISRAEL\n",
      "JERSEY CITY, NJ\n",
      "JERSUALEM, ISRAEL\n",
      "JERUSAELM, ISRAEL\n",
      "JERUSALEM, ISRAE\n",
      "JERUSALEM, ISRAEL\n",
      "JERUSALEM, ISREAL\n",
      "JERUSALEM, PALESTINIAN TERRITO\n",
      "JOHANNESBURG, SOUTH AFRICA\n",
      "JOHNANNESBURG, SOUTH AFRICA\n",
      "JOHNSTON, IA\n",
      "JUAREZ, MEXICO\n",
      "JUBA, SOUTH SUDAN\n",
      "JUNEAU, AK\n",
      "KABUL, AFGANISTAN\n",
      "KABUL, AFGHANISTAN\n",
      "KALAMAZOO, MI\n",
      "KALISPELL, MT\n",
      "KAMAKURA, JAPAN\n",
      "KAMPALA, UGANDA\n",
      "KANSAS CITY, KS\n",
      "KANSAS CITY, MO\n",
      "KATHMADU, NEPAL\n",
      "KATHMANDU, NEPAL\n",
      "KAYSERI, TURKEY\n",
      "KEMPFENHAUSEN, GERMANY\n",
      "KERICHO, KENYA\n",
      "KEY LARGO, FL\n",
      "KEY WEST, FL\n",
      "KEYSTONE, CO\n",
      "KHARGA, EGYPT\n",
      "KHARTOUM, SUDAN\n",
      "KIEV, UKRAINE\n",
      "KIGALI, REPUBLIC OF RWANDA\n",
      "KIGALI, RWANDA\n",
      "KINSALE, VA\n",
      "KINSHASA, DEMOCRATIC REPUBLIC\n",
      "KINSHASHA, DEMOCRATIC REPUBLIC\n",
      "KISUMU, KENYA\n",
      "KNOXVILLE, TN\n",
      "KRAKOW, POLAND\n",
      "KUALA LUMPUR, MALAYSIA\n",
      "KURDISTAN, IRAQ\n",
      "KUTAISI, GEORGIA\n",
      "KUWAIT CITY, KUWAIT\n",
      "KYIV, UKRAINE\n",
      "KYOTO, JAPAN\n",
      "KYRENIA, CYPRESS\n",
      "KYRENIA, CYPRUS\n",
      "KYRENIA/GIRNE, CYPRUS\n",
      "L'AQUILA, ITALY\n",
      "LA ESPERANZA, HONDURAS\n",
      "LA GUARDIA AIRPORT, NY\n",
      "LA PAZ, BOLIVIA\n",
      "LA QUINTA, CA\n",
      "LA SELVA, COSTA RICA\n",
      "LACKAWAXEN, PA\n",
      "LAGUNA BEACH, CA\n",
      "LANSING, MI\n",
      "LARNACA, CYPRUS\n",
      "LAS CRUCES, NM\n",
      "LAS VEGAS, NEVADA\n",
      "LAS VEGAS, NV\n",
      "LAYTON, NJ\n",
      "LEBANON, NH\n",
      "LEIPZIG, GERMANY\n",
      "LEXINGTON, KY\n",
      "LEXINGTON, MA\n",
      "LEXINGTON, VA\n",
      "LEXINGTON,VA\n",
      "LIBERIA, COSTA RICA\n",
      "LILONGWE, MALAWI\n",
      "LILONGWE, MILAWI\n",
      "LIMA, PERU\n",
      "LISBON, PORTUGAL\n",
      "LITTLE ROCK, AR\n",
      "LIVINGSTONE, ZAMBIA\n",
      "LONDON, ENGLAND\n",
      "LONDON, UK\n",
      "LONDON, UNITED KINGDOM\n",
      "LONDON, UNITED KINGDON\n",
      "LONG BEACH, CA\n",
      "LOS ANGELES, CA\n",
      "LOUISVILLE, KY\n",
      "LUSAKA, ZAMBIA\n",
      "LUSBY, MD\n",
      "LUTHER, MI\n",
      "LUXOR, EGYPT\n",
      "MACKINAC ISLAND, MI\n",
      "MACON, GA\n",
      "MACON, GEORGIA\n",
      "MADISON, WI\n",
      "MADRID, SPAIN\n",
      "MAGLAJ, BOSNIA AND HERZEGOVINA\n",
      "MALAKAL, SOUTH SUDAN\n",
      "MALATYA, TURKEY\n",
      "MANAMA, BAHRAIN\n",
      "MANASSAS, VA\n",
      "MANCHESTER, NH\n",
      "MANHASSET, NY\n",
      "MAPUTO, MOZAMBIQUE\n",
      "MARION COUNTY, KY\n",
      "MARRAKECH, MOROCCO\n",
      "MARRAKESH, MOROCCO\n",
      "MARSHALLTOWN, IA\n",
      "MARTHA'S VINEYARD, MA\n",
      "MASADA, ISRAEL\n",
      "MASON CITY, IA\n",
      "MATALYA, TURKEY\n",
      "MATAM, SENEGAL\n",
      "MATANZAS, CUBA\n",
      "MAUN, BOTSWANA\n",
      "MBEYA, TANZANIA\n",
      "MEDAN, INDONESIA\n",
      "MEDELLIN, COLOMBIA\n",
      "MEINE, GERMANY\n",
      "MELBOURNE, AUSTRALIA\n",
      "MEMPHIS, TN\n",
      "MERCER COUNTY, KY\n",
      "MEXICO CITY, MEXICO\n",
      "MFUWE, ZAMBIA\n",
      "MIAMI, FL\n",
      "MIDDLBURG, VA\n",
      "MIDDLEBURG, CA\n",
      "MIDDLEBURG, VA\n",
      "MIDDLEBURG, VIRGINIA\n",
      "MIDLAND, TX\n",
      "MIDWEST CITY, OK\n",
      "MILAN, ITALY\n",
      "MILWAUKEE, WI\n",
      "MINERAL, VA\n",
      "MINNEAPOLIS, MN\n",
      "MINSK, BELARUS\n",
      "MISSOULA, MT\n",
      "MOBILE, AL\n",
      "MOGADISHU, SOMALIA\n",
      "MOLINE, IL\n",
      "MONOROVIA, LIBERIA\n",
      "MONROVIA\n",
      "MONROVIA, LIBERIA\n",
      "MONTEREY, CA\n",
      "MONTERIA, COLOMBIA\n",
      "MONTREAL, CANADA\n",
      "MORRIS, IL\n",
      "MOSTAR, BOSNIA\n",
      "MOSTAR, BOSNIA AND HERZEGOVINA\n",
      "MOUNTAIN LAKES, NJ\n",
      "MOUNTAIN VIEW, CA\n",
      "MOUNTAINVIEW, CA\n",
      "MUMBAI, INDIA\n",
      "MUNICH, GERMANY\n",
      "MUSCAT, OMAN\n",
      "MUSINA, SOUTH AFRICA\n",
      "MUYINGA, BURUNDI\n",
      "MWANZA, TANZANIA\n",
      "MYRTLE BEACH, NC\n",
      "MYRTLE BEACH, SC\n",
      "N. AUGUSTA, SC\n",
      "NAGOYA, JAPAN\n",
      "NAIROBI, KENYA\n",
      "NANJING, CHINA\n",
      "NAPA, CA\n",
      "NASHVILLE TN\n",
      "NASHVILLE, TN\n",
      "NASSAU, BAHAMAS\n",
      "NAYPYITAW, MYANMAR\n",
      "NEBRASKA CITY, NE\n",
      "NECHI, COLOMBIA\n",
      "NEVADA, IA\n",
      "NEVSEHIR, TURKEY\n",
      "NEVSEHIR/GOREME, TURKEY\n",
      "NEW DEHLI, INDIA\n",
      "NEW DELHI, INDIA\n",
      "NEW DELLHI, INDIA\n",
      "NEW HAVEN, CT\n",
      "NEW ORLEANS, LA\n",
      "NEW ORLEANS, LA THIBODAUX, LA\n",
      "NEW ORLEANS, LOUISIANA\n",
      "NEW YORK ,NY\n",
      "NEW YORK CITY, NEW YORK\n",
      "NEW YORK CITY, NY\n",
      "NEW YORK, N.Y.\n",
      "NEW YORK, NEW YORK\n",
      "NEW YORK, NY\n",
      "NEW YORK. NY\n",
      "NEWARK, DE\n",
      "NEWARK, NJ\n",
      "NEWPORT BEACH, CA\n",
      "NEWTON, IA\n",
      "NIAMEY, NIGER\n",
      "NICOSIA, CYPRESS\n",
      "NICOSIA, CYPRUS\n",
      "NICOSIA/LEFKOSA, CYPRUS\n",
      "NIDGE, TURKEY\n",
      "NIGDE, TURKEY\n",
      "NOGALES, MEXICO\n",
      "NORFOLK, VA\n",
      "NORTH AUGUSTA, GA\n",
      "NORTH AUGUSTA, SC\n",
      "NORTHERN IRAQ, IRAQ\n",
      "OAK RIDGE, TN\n",
      "OAKLAND, CA\n",
      "OBERLIN, OH\n",
      "OCALA, FL\n",
      "OHRID, MACEDONIA\n",
      "OKINAWA, JAPAN\n",
      "OKLAHOMA CITY, OK\n",
      "OMAHA, NB\n",
      "OMAHA, NE\n",
      "ONEONTA, NY\n",
      "ONTARIO, CANADA\n",
      "ORADEA, ROMANIA\n",
      "ORANGE, VA\n",
      "ORANGEBURG, SC\n",
      "ORLANDO, FL\n",
      "OSAKA, JAPAN\n",
      "OSLO, NORWAY\n",
      "OWINGS MILLS, MD\n",
      "OXFORD, UNITED KINGDOM\n",
      "PALM BEACH, FL\n",
      "PALM SPRINGS, CA\n",
      "PALO ALTO, CA\n",
      "PALO, ALTO\n",
      "PALTO ALTO, CA\n",
      "PANAMA CITY, PANAMA\n",
      "PANAMA CITY, REPUBLIC OF PANAM\n",
      "PARIS, FRANCE\n",
      "PATARA, TURKEY\n",
      "PAXTON, IL\n",
      "PEORIA, IL\n",
      "PERTH, AUSTRLIA\n",
      "PETION-VILLE, HAITI\n",
      "PETIONVILLE, HAITI\n",
      "PHIADELPHIA, PA\n",
      "PHILADELPHIA,  PA\n",
      "PHILADELPHIA, PA\n",
      "PHILADEPHIA, PA\n",
      "PHILAPELPHIA, PA\n",
      "PHILDELPHIA, PA\n",
      "PHNOM PENH, CAMBODIA\n",
      "PHOENIX, AZ\n",
      "PIKESVILE, MD\n",
      "PIKESVILLE, MD\n",
      "PINE BLUFF, AR\n",
      "PITTSBURGH, PA\n",
      "PLAYA GRANDE, COSTA RICA\n",
      "PODGORICA, MONTENEGRO\n",
      "POINT CLEAR, AL\n",
      "PONCE, PUERTO RICO\n",
      "PORT AU PRINCE, HAITI\n",
      "PORT MORESBY, PAPUA NEW GUINEA\n",
      "PORT OF SPAIN, TRINIDAD AND TO\n",
      "PORT VILLA, VANUATU\n",
      "PORT-AU-PRINCE, HAITI\n",
      "PORTERVILLE, CA\n",
      "PORTLAND, OR\n",
      "PORTO, PORTUGAL\n",
      "POTSDAM, GERMANY\n",
      "PRAGUE, CZECH REPUBLIC\n",
      "PRETORIA, SOUTH AFRICA\n",
      "PRINCETON, NJ\n",
      "PRISHTINA, KOSOVO\n",
      "PRISTINA, KOSOVO\n",
      "PRISTINA, KOSOVO,\n",
      "PROVIDENCE, RI\n",
      "PROVO, UT\n",
      "PUERTO JORDAN, COLOMBIA\n",
      "PUNE, INDIA\n",
      "PUNTA CANA, DOMINICAN REPUBLIC\n",
      "QINGDAO, CHINA\n",
      "QUANTICO, VA\n",
      "QUEENS, NY\n",
      "QUEENSTOWN, MD\n",
      "QUETZALTENANGO, GUATEMALA\n",
      "RABAT, MOROCCO\n",
      "RADNOR, PA\n",
      "RALEIGH, NC\n",
      "RAMALLAH, PALESTINIAN TERRITOR\n",
      "RANGOON, BURMA\n",
      "REAGAN PRESIDENTIAL LIBRARY, C\n",
      "REDMOND, CA\n",
      "REDMOND, WA\n",
      "REDWOOD CITY, CA\n",
      "REHOBETH, DE\n",
      "REHOBOTH BEACH, DE\n",
      "REHOBOTH, DE\n",
      "RHINEBECK, NY\n",
      "RICHARD TOLL, SENEGAL\n",
      "RICHMOND, VA\n",
      "RICHMOND,VA\n",
      "RIO DE JANEIRO, BRAZIL\n",
      "RIVERTON, WY\n",
      "ROCHESTER, NY\n",
      "ROCKWELL CITY, IA\n",
      "ROME, ITALY\n",
      "ROSH PINA, ISRAEL\n",
      "ROYAL OAK, MD\n",
      "RUCKERSVILLE, VA\n",
      "SACRAMENTO, CA\n",
      "SAGINAW, MI\n",
      "SAINT LOUIS, SENEGAL\n",
      "SALEM, NJ\n",
      "SALT LAKE CITY, UT\n",
      "SAN ANTONIO, TX\n",
      "SAN DIEGO CA\n",
      "SAN DIEGO, CA\n",
      "SAN FRANCISCO, CA\n",
      "SAN FRANICISCO, CA\n",
      "SAN FRANSICO, CA\n",
      "SAN FRANSISCO, CA\n",
      "SAN JOSE, CA\n",
      "SAN JOSE, COSTA RICA\n",
      "SAN JUAN, PR\n",
      "SAN JUAN, PUERTO RICO\n",
      "SAN LUIS OBISPO, CA\n",
      "SAN PEDRO SULA, HONDURAS\n",
      "SAN PEDRO, COTE D'IVOIRE\n",
      "SAN PEDRO, HONDURAS\n",
      "SAN SALVADOR, EL SALVADOR\n",
      "SAN, DIEGO, CA\n",
      "SANLIURFA, TURKEY\n",
      "SANTA BARBARA, CA\n",
      "SANTA BARBARA, CALIFORNIA\n",
      "SANTA CLARA, CA\n",
      "SANTA FE, NM\n",
      "SANTA MONICA, CA\n",
      "SANTIAGO DE COMPOSTELA, SPAIN\n",
      "SAO PAULO, BRAZIL\n",
      "SARAJEVO, BOSNIA\n",
      "SARAJEVO, BOSNIA AND HERZEGOVI\n",
      "SARAJEVO, BOSNIA-HERZEGOVINA\n",
      "SARASOTA, FL\n",
      "SAREJEVO, BOSNIA\n",
      "SCOTTSDALE, AZ\n",
      "SCRANTON, PA\n",
      "SEA ISLAND, GA\n",
      "SEA ISLANDS, GA\n",
      "SEA OF GALILEE, ISRAEL\n",
      "SEATTE, WA\n",
      "SEATTLE, WA\n",
      "SELCUK, TURKEY\n",
      "SENDAI, JAPAN\n",
      "SEOUL, KOREA\n",
      "SEOUL, SOUTH KOREA\n",
      "SERGEANT BLUFF, IA\n",
      "SEVILLA, SPAIN\n",
      "SHANGHAI, CHINA\n",
      "SHARM EL-SHEIKH, EGYPT\n",
      "SHEFFIELD, IA\n",
      "SHELBY COUNTY, KY\n",
      "SHELBYVILLE, KY\n",
      "SHORT HILLS, NJ\n",
      "SIAYA, KENYA\n",
      "SIMI VALLEY, CA\n",
      "SINCELEJO, COLOMBIA\n",
      "SINGAPORE\n",
      "SIOUX CITY, IA\n",
      "SKOPJE, MACEDONIA\n",
      "SOLON, IA\n",
      "SOUTH BEND, IN\n",
      "SOUTH HAMILTON, MA\n",
      "SOUTH ROYALTON, VT\n",
      "SOUTH WINDSOR, CT\n",
      "SOUTHAMPTON, BERMUDA\n",
      "SOUTHPOINTE, PA\n",
      "SPENCER COUNTY, KY\n",
      "SPOKANE, WA\n",
      "SPRING CITY, TN\n",
      "SPRINGFIELD, IL\n",
      "ST. AUGUSTINE, FL\n",
      "ST. CROIX, VI\n",
      "ST. KITTS, VI\n",
      "ST. LOUIS, MO\n",
      "ST. MICHAEL'S, MD\n",
      "ST. MICHAELS, MD\n",
      "ST. PAUL, MN\n",
      "ST. THOMAS, VI\n",
      "STANFORD, CA\n",
      "STANLEY, ID\n",
      "STEAMBOAT SPRINGS, CO\n",
      "STEAMBOAT, CO\n",
      "STERLING, VA\n",
      "STEVENSVILLE, MD\n",
      "STILLWATER, OK\n",
      "STOCKHOLM, SWEDEN\n",
      "STRASBOURG, FRANCE\n",
      "STUTGART, GERMANY\n",
      "STUTTGART, GERMANY\n",
      "STUTTGART. GERMANY\n",
      "SULPHER SPRINGS, WV\n",
      "SUNNY ISLES, FL\n",
      "TAIPEI, TAIWAN\n",
      "TAL AVIV, ISRAEL\n",
      "TAMALE, GHANA\n",
      "TAMPA, FL\n",
      "TAMUNING, GUAM\n",
      "TAPACHULA, MEXICO\n",
      "TARRYTOWN, NY\n",
      "TBILISI, GEORGIA\n",
      "TBLISI, GEORGIA\n",
      "TECPAN, GUATEMALA\n",
      "TEGUCIGALPA, HONDURAS\n",
      "TEL AVIV, ISRAEL\n",
      "TEL AVIV, ISREAL\n",
      "TEL-AVIV, ISRAEL\n",
      "TELA, HONDURAS\n",
      "TELAVI, GEORGIA\n",
      "THE HYATT REGENCY CHESAPEAKE B\n",
      "THIBADAUX, LA\n",
      "THIBODAUX, LA\n",
      "THORNTON, IA\n",
      "THURMONT, MD\n",
      "TIBERAIS, ISRAEL\n",
      "TIBERAS, ISRAEL\n",
      "TIBERIAS\n",
      "TIBERIAS, ISRAEL\n",
      "TIBERIAS, ISREAL\n",
      "TIBERIUS, ISRAEL\n",
      "TIBRIAS, ISRAEL\n",
      "TINDOUF, ALGERIA\n",
      "TIRANA, ALBANIA\n",
      "TOHOKU, JAPAN\n",
      "TOKYO, JAPAN\n",
      "TORONTO, CANADA\n",
      "TORTUGUERO, COSTA RICA\n",
      "TOTONICAPAN, GUATEMALA\n",
      "TOTONICIPAN, GUATEMALA\n",
      "TOWSON, MD\n",
      "TOYKO, JAPAN\n",
      "TRABZON, TURKEY\n",
      "TULSA, OK\n",
      "TUNICA, MI\n",
      "TUNICA, MS\n",
      "TUNIS, TUNISIA\n",
      "TUNKHANNOCK, PA\n",
      "TURIN, ITALY\n",
      "TURRIALBA, COSTA RICA\n",
      "ULAANBAATAR, MONGOLIA\n",
      "UNION BRIDGE, MD\n",
      "URBANDALE, IA\n",
      "URBANDALE,IA\n",
      "VACHERIE, LA\n",
      "VALDEZ, AK\n",
      "VANCOUVER, CANADA\n",
      "VIENNA, AUSTRIA\n",
      "VILLEPINTE, FRANCE\n",
      "VILLIPINTE, FRANCE\n",
      "VIRGINIA BEACH, VA\n",
      "VISALIA, CA\n",
      "VLAANBAATAR, MONGOLIA\n",
      "WAIMEA, HAWAII\n",
      "WALL LAKE, IA\n",
      "WARREN, NJ\n",
      "WARRENTON, VA\n",
      "WARRENTON,VA\n",
      "WARSAW, POLAND\n",
      "WASHINGTON, DC\n",
      "WASHINGTON, IA\n",
      "WAYNESBORO, GA\n",
      "WENHAM, MA\n",
      "WEST LAKE VILLAGE, CA\n",
      "WEST PALM BEACH\n",
      "WEST PALM BEACH, FL\n",
      "WEST PALM PEACH, FL\n",
      "WEST POINT, NY\n",
      "WESTLAKE VILLAGE, CA\n",
      "WHEELING, WV\n",
      "WHITE HALL, MD\n",
      "WHITE SULPHUR SPRINGS, WV\n",
      "WICHITA, KS\n",
      "WIITENBERG, GERMANY\n",
      "WILILAMSBURG, VA\n",
      "WILLIAMSBURG, VA\n",
      "WILLIMASBURG, VA\n",
      "WILMINGTON, DE\n",
      "WITCHITA, KS\n",
      "WITTENBERG, GERMANY\n",
      "WOOSTER, OH\n",
      "WROCLAW, POLAND\n",
      "WYTHEVILLE, VA\n",
      "XI'AN, CHINA\n",
      "YAMBIO, SOUTH SUDAN\n",
      "YANGON, MYANMAR\n",
      "YAOUNDE, CAMEROON\n",
      "YEREVAN, ARMENIA\n",
      "ZAGREB, CROATIA\n",
      "ZANZIBAR, TANZANIA\n",
      "ZHYTOMYR, UKRAINE\n",
      "ZUGDIDI, GEORGIA\n",
      "ZURICH, SWITZERLAND\n"
     ]
    }
   ],
   "source": [
    "for destination in sorted(junkets.destination_clean.unique()):\n",
    "    print(destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is where we're going to start encoding our editorial choices. \"Ames, IA\" or \"Ames, Iowa\"? \"Baku, Azerjaijan,\" or \"Baku, Republic of Azerbaijan\"? Etc.\n",
    "\n",
    "There are several ways we could structure this data, but a dictionary sounds like it'd be the most fun, so let's do that. Each key will be a string that we'd like to replace; each value will be the string we'd like to replace it with. To get us started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "typo_fixes = {\n",
    "    'BAKU, AZERBIJAN': 'BAKU, AZERBAIJAN',\n",
    "    'BAKU, REPUBLIC OF AZERBAIJAN': 'BAKU, AZERBAIJAN',\n",
    "    'ADDIS, ETHIOPIA': 'ADDIS ABABA, ETHIOPIA',\n",
    "    'ANKEY, IA': 'ANKENY, IA'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and so on. (This is tedious work, and -- again -- tools like OpenRefine make this process somewhat less tedious. But if you have a long-term project that involves data that will be updated regularly, and it's worth putting in the time to make sure the data are cleaned the same way each time, you can do it all in pandas.)\n",
    "\n",
    "👉 For more information on dictionaries, [check out this notebook](Python%20data%20types%20and%20basic%20syntax.ipynb#Dictionaries).\n",
    "\n",
    "Here's how we might _apply_ our bulk find-and-replace dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_replace_destination(row):\n",
    "    '''Given a row of data, see if the value is a typo to be replaced'''\n",
    "    \n",
    "    # get the clean destination value\n",
    "    dest = row['destination_clean']\n",
    "    \n",
    "    # try to look it up in the `typo_fixes` dictionary\n",
    "    # the `get()` method will return None if it's not there\n",
    "    typo = typo_fixes.get(dest)\n",
    "    \n",
    "    # then we can test to see if `get()` got an item out of the dictionary (True)\n",
    "    # or if it returned None (False)\n",
    "    if typo:\n",
    "        # if it found an entry in our dictionary,\n",
    "        # return the value from that key/value pair\n",
    "        return typo_fixes[dest]\n",
    "    # otherwise\n",
    "    else:\n",
    "        # return the original destination string\n",
    "        return dest\n",
    "\n",
    "# apply the function and overwrite our working \"clean' column\"\n",
    "junkets['destination_clean'] = junkets.apply(find_replace_destination, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocID</th>\n",
       "      <th>FilerName</th>\n",
       "      <th>MemberName</th>\n",
       "      <th>State</th>\n",
       "      <th>District</th>\n",
       "      <th>Year</th>\n",
       "      <th>Destination</th>\n",
       "      <th>FilingType</th>\n",
       "      <th>DepartureDate</th>\n",
       "      <th>ReturnDate</th>\n",
       "      <th>TravelSponsor</th>\n",
       "      <th>destination_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500005076</td>\n",
       "      <td>Bobby Cornett</td>\n",
       "      <td>Franks, Trent</td>\n",
       "      <td>AZ</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>Original</td>\n",
       "      <td>1/7/2011</td>\n",
       "      <td>1/9/2011</td>\n",
       "      <td>Consumer Electronics Association</td>\n",
       "      <td>LAS VEGAS, NV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500005077</td>\n",
       "      <td>Michael Strittmatter</td>\n",
       "      <td>Franks, Trent</td>\n",
       "      <td>AZ</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>Original</td>\n",
       "      <td>1/7/2011</td>\n",
       "      <td>1/9/2011</td>\n",
       "      <td>CEA Leaders in Technology</td>\n",
       "      <td>LAS VEGAS, NV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500005081</td>\n",
       "      <td>Diane Rinaldo</td>\n",
       "      <td>Rogers, Mike</td>\n",
       "      <td>AL</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>Original</td>\n",
       "      <td>1/6/2011</td>\n",
       "      <td>1/8/2011</td>\n",
       "      <td>Consumer Electronics Association</td>\n",
       "      <td>LAS VEGAS, NV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500005082</td>\n",
       "      <td>Kenneth DeGraff</td>\n",
       "      <td>Doyle, Michael</td>\n",
       "      <td>PA</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>Original</td>\n",
       "      <td>1/6/2011</td>\n",
       "      <td>1/9/2011</td>\n",
       "      <td>Consumer Electronics Association</td>\n",
       "      <td>LAS VEGAS, NV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500005083</td>\n",
       "      <td>Michael Ryan Clough</td>\n",
       "      <td>Lofgren, Zoe</td>\n",
       "      <td>CA</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>Original</td>\n",
       "      <td>1/6/2011</td>\n",
       "      <td>1/8/2011</td>\n",
       "      <td>Consumer Electronics Association</td>\n",
       "      <td>LAS VEGAS, NV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DocID             FilerName      MemberName State  District  Year  \\\n",
       "0  500005076         Bobby Cornett   Franks, Trent    AZ       8.0  2011   \n",
       "1  500005077  Michael Strittmatter   Franks, Trent    AZ       8.0  2011   \n",
       "2  500005081         Diane Rinaldo    Rogers, Mike    AL       3.0  2011   \n",
       "3  500005082       Kenneth DeGraff  Doyle, Michael    PA      14.0  2011   \n",
       "4  500005083   Michael Ryan Clough    Lofgren, Zoe    CA      19.0  2011   \n",
       "\n",
       "     Destination FilingType DepartureDate ReturnDate  \\\n",
       "0  Las Vegas, NV   Original      1/7/2011   1/9/2011   \n",
       "1  Las Vegas, NV   Original      1/7/2011   1/9/2011   \n",
       "2  Las Vegas, NV   Original      1/6/2011   1/8/2011   \n",
       "3  Las Vegas, NV   Original      1/6/2011   1/9/2011   \n",
       "4  Las Vegas, NV   Original      1/6/2011   1/8/2011   \n",
       "\n",
       "                      TravelSponsor destination_clean  \n",
       "0  Consumer Electronics Association     LAS VEGAS, NV  \n",
       "1         CEA Leaders in Technology     LAS VEGAS, NV  \n",
       "2  Consumer Electronics Association     LAS VEGAS, NV  \n",
       "3  Consumer Electronics Association     LAS VEGAS, NV  \n",
       "4  Consumer Electronics Association     LAS VEGAS, NV  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "junkets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 For more information on writing your own functions, [check out this notebook](Functions.ipynb).\n",
    "\n",
    "👉 For more information on applying functions to a pandas data frame, [check out this notebook](Using%20the%20apply%20method%20in%20pandas.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonstandard values to represent null entries\n",
    "\n",
    "Data creators may express null values in a variety of ways -- `''`, `'n/a'`, `NA`, `.`, etc. But for your purposes, you want pandas to read them all as `NaN`, so you can take advantage of methods like [`isnull()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html) in your analysis.\n",
    "\n",
    "If you've already done some exploratory analysis, you can specify the `na_values` argument when you read in the data -- you can supply a _single_ value that should be interpreted as null, or you can hand off a list `[]` of values.\n",
    "\n",
    "As an example, let's take a look at some EPA air quality data from Ohio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality = pd.read_excel('../data/epa.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CBSA Code</th>\n",
       "      <th>CBSA</th>\n",
       "      <th>CO 2nd Max 1-hr</th>\n",
       "      <th>CO 2nd Max 8-hr</th>\n",
       "      <th>NO2 98th Percentile 1-hr</th>\n",
       "      <th>NO2 Mean 1-hr</th>\n",
       "      <th>Ozone 2nd Max 1-hr</th>\n",
       "      <th>Ozone 4th Max 8-hr</th>\n",
       "      <th>SO2 99th Percentile 1-hr</th>\n",
       "      <th>SO2 2nd Max 24-hr</th>\n",
       "      <th>SO2 Mean 1-hr</th>\n",
       "      <th>PM2.5 98th Percentile 24-hr</th>\n",
       "      <th>PM2.5 Weighted Mean 24-hr</th>\n",
       "      <th>PM10 2nd Max 24-hr</th>\n",
       "      <th>PM10 Mean 24-hr</th>\n",
       "      <th>Lead Max 3-Mo Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10420</td>\n",
       "      <td>Akron, OH</td>\n",
       "      <td>17.6</td>\n",
       "      <td>7.8</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.099</td>\n",
       "      <td>188</td>\n",
       "      <td>66</td>\n",
       "      <td>19</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11780</td>\n",
       "      <td>Ashtabula, OH</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.092</td>\n",
       "      <td>101</td>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15940</td>\n",
       "      <td>Canton-Massillon, OH</td>\n",
       "      <td>8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>.</td>\n",
       "      <td>29</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.087</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17060</td>\n",
       "      <td>Chillicothe, OH</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17140</td>\n",
       "      <td>Cincinnati, OH-KY-IN</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>70</td>\n",
       "      <td>26</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.11</td>\n",
       "      <td>190</td>\n",
       "      <td>55</td>\n",
       "      <td>14</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CBSA Code                  CBSA CO 2nd Max 1-hr CO 2nd Max 8-hr  \\\n",
       "0      10420             Akron, OH            17.6             7.8   \n",
       "1      11780         Ashtabula, OH               .               .   \n",
       "2      15940  Canton-Massillon, OH               8             4.5   \n",
       "3      17060       Chillicothe, OH               .               .   \n",
       "4      17140  Cincinnati, OH-KY-IN              10               6   \n",
       "\n",
       "  NO2 98th Percentile 1-hr NO2 Mean 1-hr Ozone 2nd Max 1-hr  \\\n",
       "0                        .             .               0.12   \n",
       "1                        .             .               0.11   \n",
       "2                        .            29                0.1   \n",
       "3                        .             .                  .   \n",
       "4                       70            26               0.16   \n",
       "\n",
       "  Ozone 4th Max 8-hr SO2 99th Percentile 1-hr SO2 2nd Max 24-hr SO2 Mean 1-hr  \\\n",
       "0              0.099                      188                66            19   \n",
       "1              0.092                      101                34            10   \n",
       "2              0.087                        .                 .             .   \n",
       "3                  .                        .                 .             .   \n",
       "4               0.11                      190                55            14   \n",
       "\n",
       "  PM2.5 98th Percentile 24-hr PM2.5 Weighted Mean 24-hr PM10 2nd Max 24-hr  \\\n",
       "0                           .                         .                  .   \n",
       "1                           .                         .                  .   \n",
       "2                           .                         .                  .   \n",
       "3                           .                         .                  .   \n",
       "4                           .                         .                  .   \n",
       "\n",
       "  PM10 Mean 24-hr Lead Max 3-Mo Avg  \n",
       "0               .                 .  \n",
       "1               .                 .  \n",
       "2               .                 .  \n",
       "3               .                 .  \n",
       "4               .                 .  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_quality.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After conferring with the source of this data, the dots `.` represent \"no observation\" -- a null value. Let's try reading that in again, this time specifying `na_values`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality = pd.read_excel('../data/epa.xlsx',\n",
    "                            na_values='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CBSA Code</th>\n",
       "      <th>CBSA</th>\n",
       "      <th>CO 2nd Max 1-hr</th>\n",
       "      <th>CO 2nd Max 8-hr</th>\n",
       "      <th>NO2 98th Percentile 1-hr</th>\n",
       "      <th>NO2 Mean 1-hr</th>\n",
       "      <th>Ozone 2nd Max 1-hr</th>\n",
       "      <th>Ozone 4th Max 8-hr</th>\n",
       "      <th>SO2 99th Percentile 1-hr</th>\n",
       "      <th>SO2 2nd Max 24-hr</th>\n",
       "      <th>SO2 Mean 1-hr</th>\n",
       "      <th>PM2.5 98th Percentile 24-hr</th>\n",
       "      <th>PM2.5 Weighted Mean 24-hr</th>\n",
       "      <th>PM10 2nd Max 24-hr</th>\n",
       "      <th>PM10 Mean 24-hr</th>\n",
       "      <th>Lead Max 3-Mo Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10420</td>\n",
       "      <td>Akron, OH</td>\n",
       "      <td>17.6</td>\n",
       "      <td>7.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.099</td>\n",
       "      <td>188.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11780</td>\n",
       "      <td>Ashtabula, OH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.092</td>\n",
       "      <td>101.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15940</td>\n",
       "      <td>Canton-Massillon, OH</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17060</td>\n",
       "      <td>Chillicothe, OH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17140</td>\n",
       "      <td>Cincinnati, OH-KY-IN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.110</td>\n",
       "      <td>190.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CBSA Code                  CBSA  CO 2nd Max 1-hr  CO 2nd Max 8-hr  \\\n",
       "0      10420             Akron, OH             17.6              7.8   \n",
       "1      11780         Ashtabula, OH              NaN              NaN   \n",
       "2      15940  Canton-Massillon, OH              8.0              4.5   \n",
       "3      17060       Chillicothe, OH              NaN              NaN   \n",
       "4      17140  Cincinnati, OH-KY-IN             10.0              6.0   \n",
       "\n",
       "   NO2 98th Percentile 1-hr  NO2 Mean 1-hr  Ozone 2nd Max 1-hr  \\\n",
       "0                       NaN            NaN                0.12   \n",
       "1                       NaN            NaN                0.11   \n",
       "2                       NaN           29.0                0.10   \n",
       "3                       NaN            NaN                 NaN   \n",
       "4                      70.0           26.0                0.16   \n",
       "\n",
       "   Ozone 4th Max 8-hr  SO2 99th Percentile 1-hr  SO2 2nd Max 24-hr  \\\n",
       "0               0.099                     188.0               66.0   \n",
       "1               0.092                     101.0               34.0   \n",
       "2               0.087                       NaN                NaN   \n",
       "3                 NaN                       NaN                NaN   \n",
       "4               0.110                     190.0               55.0   \n",
       "\n",
       "   SO2 Mean 1-hr  PM2.5 98th Percentile 24-hr  PM2.5 Weighted Mean 24-hr  \\\n",
       "0           19.0                          NaN                        NaN   \n",
       "1           10.0                          NaN                        NaN   \n",
       "2            NaN                          NaN                        NaN   \n",
       "3            NaN                          NaN                        NaN   \n",
       "4           14.0                          NaN                        NaN   \n",
       "\n",
       "   PM10 2nd Max 24-hr  PM10 Mean 24-hr  Lead Max 3-Mo Avg  \n",
       "0                 NaN              NaN                NaN  \n",
       "1                 NaN              NaN                NaN  \n",
       "2                 NaN              NaN                NaN  \n",
       "3                 NaN              NaN                NaN  \n",
       "4                 NaN              NaN                NaN  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_quality.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You want to replace null values with 0, or something else\n",
    "\n",
    "Use the [`fillna()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html) method on a data frame or column series to fill null values with some other value.\n",
    "\n",
    "Let's say our reporting had shown that the dots in the air quality data weren't, in fact null. Let's say they were actually supposed to be zeroes. Here's how we'd fix that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CBSA Code</th>\n",
       "      <th>CBSA</th>\n",
       "      <th>CO 2nd Max 1-hr</th>\n",
       "      <th>CO 2nd Max 8-hr</th>\n",
       "      <th>NO2 98th Percentile 1-hr</th>\n",
       "      <th>NO2 Mean 1-hr</th>\n",
       "      <th>Ozone 2nd Max 1-hr</th>\n",
       "      <th>Ozone 4th Max 8-hr</th>\n",
       "      <th>SO2 99th Percentile 1-hr</th>\n",
       "      <th>SO2 2nd Max 24-hr</th>\n",
       "      <th>SO2 Mean 1-hr</th>\n",
       "      <th>PM2.5 98th Percentile 24-hr</th>\n",
       "      <th>PM2.5 Weighted Mean 24-hr</th>\n",
       "      <th>PM10 2nd Max 24-hr</th>\n",
       "      <th>PM10 Mean 24-hr</th>\n",
       "      <th>Lead Max 3-Mo Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10420</td>\n",
       "      <td>Akron, OH</td>\n",
       "      <td>17.6</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.099</td>\n",
       "      <td>188.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11780</td>\n",
       "      <td>Ashtabula, OH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.092</td>\n",
       "      <td>101.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15940</td>\n",
       "      <td>Canton-Massillon, OH</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17060</td>\n",
       "      <td>Chillicothe, OH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17140</td>\n",
       "      <td>Cincinnati, OH-KY-IN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.110</td>\n",
       "      <td>190.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17460</td>\n",
       "      <td>Cleveland-Elyria, OH</td>\n",
       "      <td>16.3</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.097</td>\n",
       "      <td>516.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18140</td>\n",
       "      <td>Columbus, OH</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.101</td>\n",
       "      <td>173.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18740</td>\n",
       "      <td>Coshocton, OH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19380</td>\n",
       "      <td>Dayton, OH</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22300</td>\n",
       "      <td>Findlay, OH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>26580</td>\n",
       "      <td>Huntington-Ashland, WV-KY-OH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>30620</td>\n",
       "      <td>Lima, OH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>31900</td>\n",
       "      <td>Mansfield, OH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>31930</td>\n",
       "      <td>Marietta, OH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>32020</td>\n",
       "      <td>Marion, OH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>41400</td>\n",
       "      <td>Salem, OH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>41780</td>\n",
       "      <td>Sandusky, OH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>44220</td>\n",
       "      <td>Springfield, OH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>45660</td>\n",
       "      <td>Tiffin, OH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>45780</td>\n",
       "      <td>Toledo, OH</td>\n",
       "      <td>12.5</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.101</td>\n",
       "      <td>255.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>48260</td>\n",
       "      <td>Weirton-Steubenville, WV-OH</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>75.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.107</td>\n",
       "      <td>280.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>48540</td>\n",
       "      <td>Wheeling, WV-OH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>49660</td>\n",
       "      <td>Youngstown-Warren-Boardman, OH-PA</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.091</td>\n",
       "      <td>185.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CBSA Code                               CBSA  CO 2nd Max 1-hr  \\\n",
       "0       10420                          Akron, OH             17.6   \n",
       "1       11780                      Ashtabula, OH              0.0   \n",
       "2       15940               Canton-Massillon, OH              8.0   \n",
       "3       17060                    Chillicothe, OH              0.0   \n",
       "4       17140               Cincinnati, OH-KY-IN             10.0   \n",
       "5       17460               Cleveland-Elyria, OH             16.3   \n",
       "6       18140                       Columbus, OH             20.0   \n",
       "7       18740                      Coshocton, OH              0.0   \n",
       "8       19380                         Dayton, OH             14.0   \n",
       "9       22300                        Findlay, OH              0.0   \n",
       "10      26580       Huntington-Ashland, WV-KY-OH              0.0   \n",
       "11      30620                           Lima, OH              0.0   \n",
       "12      31900                      Mansfield, OH              0.0   \n",
       "13      31930                       Marietta, OH              0.0   \n",
       "14      32020                         Marion, OH              0.0   \n",
       "15      41400                          Salem, OH              0.0   \n",
       "16      41780                       Sandusky, OH              0.0   \n",
       "17      44220                    Springfield, OH              0.0   \n",
       "18      45660                         Tiffin, OH              0.0   \n",
       "19      45780                         Toledo, OH             12.5   \n",
       "20      48260        Weirton-Steubenville, WV-OH             12.0   \n",
       "21      48540                    Wheeling, WV-OH              0.0   \n",
       "22      49660  Youngstown-Warren-Boardman, OH-PA             11.0   \n",
       "\n",
       "    CO 2nd Max 8-hr  NO2 98th Percentile 1-hr  NO2 Mean 1-hr  \\\n",
       "0               7.8                       0.0            0.0   \n",
       "1               0.0                       0.0            0.0   \n",
       "2               4.5                       0.0           29.0   \n",
       "3               0.0                       0.0            0.0   \n",
       "4               6.0                      70.0           26.0   \n",
       "5              11.0                       0.0            0.0   \n",
       "6              12.1                       0.0           19.0   \n",
       "7               0.0                       0.0            0.0   \n",
       "8               7.1                       0.0            0.0   \n",
       "9               0.0                       0.0            0.0   \n",
       "10              9.1                       0.0            0.0   \n",
       "11              0.0                       0.0            0.0   \n",
       "12              0.0                       0.0            0.0   \n",
       "13              0.0                       0.0            0.0   \n",
       "14              0.0                       0.0            0.0   \n",
       "15              0.0                       0.0            0.0   \n",
       "16              0.0                       0.0            0.0   \n",
       "17              0.0                       0.0            0.0   \n",
       "18              0.0                       0.0            0.0   \n",
       "19              5.8                       0.0            0.0   \n",
       "20              8.3                      75.0           23.0   \n",
       "21              4.9                       0.0            0.0   \n",
       "22              6.0                     175.0           41.0   \n",
       "\n",
       "    Ozone 2nd Max 1-hr  Ozone 4th Max 8-hr  SO2 99th Percentile 1-hr  \\\n",
       "0                 0.12               0.099                     188.0   \n",
       "1                 0.11               0.092                     101.0   \n",
       "2                 0.10               0.087                       0.0   \n",
       "3                 0.00               0.000                       0.0   \n",
       "4                 0.16               0.110                     190.0   \n",
       "5                 0.12               0.097                     516.0   \n",
       "6                 0.13               0.101                     173.0   \n",
       "7                 0.00               0.000                       0.0   \n",
       "8                 0.13               0.102                       0.0   \n",
       "9                 0.00               0.000                       0.0   \n",
       "10                0.13               0.093                       0.0   \n",
       "11                0.00               0.000                       0.0   \n",
       "12                0.00               0.000                       0.0   \n",
       "13                0.00               0.000                       0.0   \n",
       "14                0.00               0.000                       0.0   \n",
       "15                0.00               0.000                       0.0   \n",
       "16                0.00               0.000                       0.0   \n",
       "17                0.13               0.094                       0.0   \n",
       "18                0.00               0.000                       0.0   \n",
       "19                0.14               0.101                     255.0   \n",
       "20                0.16               0.107                     280.0   \n",
       "21                0.09               0.062                       0.0   \n",
       "22                0.12               0.091                     185.0   \n",
       "\n",
       "    SO2 2nd Max 24-hr  SO2 Mean 1-hr  PM2.5 98th Percentile 24-hr  \\\n",
       "0                66.0           19.0                          0.0   \n",
       "1                34.0           10.0                          0.0   \n",
       "2                 0.0            0.0                          0.0   \n",
       "3                 0.0            0.0                          0.0   \n",
       "4                55.0           14.0                          0.0   \n",
       "5               125.0           18.0                          0.0   \n",
       "6                48.0            9.0                          0.0   \n",
       "7                 0.0            0.0                          0.0   \n",
       "8                 0.0            0.0                          0.0   \n",
       "9                 0.0            0.0                          0.0   \n",
       "10                0.0            0.0                          0.0   \n",
       "11                0.0            0.0                          0.0   \n",
       "12                0.0            0.0                          0.0   \n",
       "13                0.0            0.0                          0.0   \n",
       "14                0.0            0.0                          0.0   \n",
       "15                0.0            0.0                          0.0   \n",
       "16                0.0            0.0                          0.0   \n",
       "17                0.0            0.0                          0.0   \n",
       "18                0.0            0.0                          0.0   \n",
       "19               86.0           13.0                          0.0   \n",
       "20               86.0           27.0                          0.0   \n",
       "21                0.0            0.0                          0.0   \n",
       "22               60.0           13.0                          0.0   \n",
       "\n",
       "    PM2.5 Weighted Mean 24-hr  PM10 2nd Max 24-hr  PM10 Mean 24-hr  \\\n",
       "0                         0.0                 0.0              0.0   \n",
       "1                         0.0                 0.0              0.0   \n",
       "2                         0.0                 0.0              0.0   \n",
       "3                         0.0                 0.0              0.0   \n",
       "4                         0.0                 0.0              0.0   \n",
       "5                         0.0                 0.0              0.0   \n",
       "6                         0.0                 0.0              0.0   \n",
       "7                         0.0                 0.0              0.0   \n",
       "8                         0.0                 0.0              0.0   \n",
       "9                         0.0                 0.0              0.0   \n",
       "10                        0.0                 0.0              0.0   \n",
       "11                        0.0                 0.0              0.0   \n",
       "12                        0.0                 0.0              0.0   \n",
       "13                        0.0                 0.0              0.0   \n",
       "14                        0.0                 0.0              0.0   \n",
       "15                        0.0                 0.0              0.0   \n",
       "16                        0.0                 0.0              0.0   \n",
       "17                        0.0                 0.0              0.0   \n",
       "18                        0.0                 0.0              0.0   \n",
       "19                        0.0                 0.0              0.0   \n",
       "20                        0.0                 0.0              0.0   \n",
       "21                        0.0                 0.0              0.0   \n",
       "22                        0.0                 0.0              0.0   \n",
       "\n",
       "    Lead Max 3-Mo Avg  \n",
       "0                 0.0  \n",
       "1                 0.0  \n",
       "2                 0.0  \n",
       "3                 0.0  \n",
       "4                 0.0  \n",
       "5                 0.0  \n",
       "6                 0.0  \n",
       "7                 0.0  \n",
       "8                 0.0  \n",
       "9                 0.0  \n",
       "10                0.0  \n",
       "11                0.0  \n",
       "12                0.0  \n",
       "13                0.0  \n",
       "14                0.0  \n",
       "15                0.0  \n",
       "16                0.0  \n",
       "17                0.0  \n",
       "18                0.0  \n",
       "19                0.0  \n",
       "20                0.0  \n",
       "21                0.0  \n",
       "22                0.0  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_quality.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You have duplicate rows\n",
    "\n",
    "If your data have rows that are incorrectly duplicated, you use the data frame method [`drop_duplicates()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html) to delete the duplicates.\n",
    "\n",
    "(This assumes, of course, that you've done sufficient reporting to feel confident that the duplicated rows aren't in there legitimately.)\n",
    "\n",
    "Let's look at some fake data to show how this'd work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = [\n",
    "    {'id': 12345, 'name': 'Sally', 'position': 'Editor', 'org': 'Some News Organization'},\n",
    "    {'id': 54321, 'name': 'George', 'position': 'Reporter', 'org': 'Some Other News Organization'},\n",
    "    {'id': 12345, 'name': 'Sally', 'position': 'Editor', 'org': 'Some News Organization'},\n",
    "    {'id': 49382, 'name': 'Sally', 'position': 'Editor', 'org': 'Some News Organization'},\n",
    "    {'id': 39331, 'name': 'Pat', 'position': 'Producer', 'org': 'Some Other Other News Organization'},\n",
    "]\n",
    "\n",
    "fake_df = pd.DataFrame(fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>org</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12345</td>\n",
       "      <td>Sally</td>\n",
       "      <td>Some News Organization</td>\n",
       "      <td>Editor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54321</td>\n",
       "      <td>George</td>\n",
       "      <td>Some Other News Organization</td>\n",
       "      <td>Reporter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12345</td>\n",
       "      <td>Sally</td>\n",
       "      <td>Some News Organization</td>\n",
       "      <td>Editor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49382</td>\n",
       "      <td>Sally</td>\n",
       "      <td>Some News Organization</td>\n",
       "      <td>Editor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39331</td>\n",
       "      <td>Pat</td>\n",
       "      <td>Some Other Other News Organization</td>\n",
       "      <td>Producer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    name                                 org  position\n",
       "0  12345   Sally              Some News Organization    Editor\n",
       "1  54321  George        Some Other News Organization  Reporter\n",
       "2  12345   Sally              Some News Organization    Editor\n",
       "3  49382   Sally              Some News Organization    Editor\n",
       "4  39331     Pat  Some Other Other News Organization  Producer"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you drop anything, you'd probably want to check for duplicate rows. You can do that by filtering your data using the `duplicated()` method.\n",
    "\n",
    "👉 For more details on filtering data in pandas, [see this notebook](Filtering%20columns%20and%20rows%20in%20pandas.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>org</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12345</td>\n",
       "      <td>Sally</td>\n",
       "      <td>Some News Organization</td>\n",
       "      <td>Editor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id   name                     org position\n",
       "2  12345  Sally  Some News Organization   Editor"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_df[fake_df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is showing us a row where every value in every column matches exactly the values in at least one other row. So we've done the reporting to show that we need to cut the duplicates here.\n",
    "\n",
    "The `drop_duplicates()` method gives you control over _how_ this happens:\n",
    "- You can drop _all_ duplicate rows, or keep just the first instance (this is the default behavior), or the last instance\n",
    "- You can drop rows where just the values in certain columns are duplicated\n",
    "\n",
    "Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>org</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12345</td>\n",
       "      <td>Sally</td>\n",
       "      <td>Some News Organization</td>\n",
       "      <td>Editor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54321</td>\n",
       "      <td>George</td>\n",
       "      <td>Some Other News Organization</td>\n",
       "      <td>Reporter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49382</td>\n",
       "      <td>Sally</td>\n",
       "      <td>Some News Organization</td>\n",
       "      <td>Editor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39331</td>\n",
       "      <td>Pat</td>\n",
       "      <td>Some Other Other News Organization</td>\n",
       "      <td>Producer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    name                                 org  position\n",
       "0  12345   Sally              Some News Organization    Editor\n",
       "1  54321  George        Some Other News Organization  Reporter\n",
       "3  49382   Sally              Some News Organization    Editor\n",
       "4  39331     Pat  Some Other Other News Organization  Producer"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default behavior -- duplicate rows must match exactly\n",
    "fake_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>org</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12345</td>\n",
       "      <td>Sally</td>\n",
       "      <td>Some News Organization</td>\n",
       "      <td>Editor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54321</td>\n",
       "      <td>George</td>\n",
       "      <td>Some Other News Organization</td>\n",
       "      <td>Reporter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39331</td>\n",
       "      <td>Pat</td>\n",
       "      <td>Some Other Other News Organization</td>\n",
       "      <td>Producer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    name                                 org  position\n",
       "0  12345   Sally              Some News Organization    Editor\n",
       "1  54321  George        Some Other News Organization  Reporter\n",
       "4  39331     Pat  Some Other Other News Organization  Producer"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop rows where the values in name, org and position are identical\n",
    "fake_df.drop_duplicates(subset=['name', 'org', 'position'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our original data frame is unchanged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>org</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12345</td>\n",
       "      <td>Sally</td>\n",
       "      <td>Some News Organization</td>\n",
       "      <td>Editor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54321</td>\n",
       "      <td>George</td>\n",
       "      <td>Some Other News Organization</td>\n",
       "      <td>Reporter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12345</td>\n",
       "      <td>Sally</td>\n",
       "      <td>Some News Organization</td>\n",
       "      <td>Editor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49382</td>\n",
       "      <td>Sally</td>\n",
       "      <td>Some News Organization</td>\n",
       "      <td>Editor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39331</td>\n",
       "      <td>Pat</td>\n",
       "      <td>Some Other Other News Organization</td>\n",
       "      <td>Producer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    name                                 org  position\n",
       "0  12345   Sally              Some News Organization    Editor\n",
       "1  54321  George        Some Other News Organization  Reporter\n",
       "2  12345   Sally              Some News Organization    Editor\n",
       "3  49382   Sally              Some News Organization    Editor\n",
       "4  39331     Pat  Some Other Other News Organization  Producer"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's because we didn't specify `in_place=True` as an argument.\n",
    "\n",
    "You can take one of two approaches here. You could alter your original dataframe -- the code would look like this:\n",
    "\n",
    "```python\n",
    "# drop rows where the values in name, org and position are identical\n",
    "fake_df.drop_duplicates(subset=['name', 'org', 'position'], inplace=True)\n",
    "```\n",
    "\n",
    "-- or you could \"save\" the resulting deduplicated data frame as a new variable, like this:\n",
    "\n",
    "```python\n",
    "# drop rows where the values in name, org and position are identical\n",
    "deduped = fake_df.drop_duplicates(subset=['name', 'org', 'position'])\n",
    "```\n",
    "\n",
    "I prefer the latter approach because I like to leave the original data as untouched as possible, working up to successively cleaner and more analyze-able data frames as I go. I also find this approach is easier to follow when I come back to it after a few weeks or months of inaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You have empty rows\n",
    "\n",
    "To drop rows or columns whose values are all `NA`, use [`dropna()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html).\n",
    "\n",
    "Specifying `axis=1` will drop empty _columns_; `axis=0` will drop empty _rows_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fake_df = fake_df.append({'id': np.nan, 'name': np.nan, 'position': np.nan, 'org': np.nan}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>org</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12345.0</td>\n",
       "      <td>Sally</td>\n",
       "      <td>Some News Organization</td>\n",
       "      <td>Editor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54321.0</td>\n",
       "      <td>George</td>\n",
       "      <td>Some Other News Organization</td>\n",
       "      <td>Reporter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12345.0</td>\n",
       "      <td>Sally</td>\n",
       "      <td>Some News Organization</td>\n",
       "      <td>Editor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49382.0</td>\n",
       "      <td>Sally</td>\n",
       "      <td>Some News Organization</td>\n",
       "      <td>Editor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39331.0</td>\n",
       "      <td>Pat</td>\n",
       "      <td>Some Other Other News Organization</td>\n",
       "      <td>Producer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    name                                 org  position\n",
       "0  12345.0   Sally              Some News Organization    Editor\n",
       "1  54321.0  George        Some Other News Organization  Reporter\n",
       "2  12345.0   Sally              Some News Organization    Editor\n",
       "3  49382.0   Sally              Some News Organization    Editor\n",
       "4  39331.0     Pat  Some Other Other News Organization  Producer\n",
       "5      NaN     NaN                                 NaN       NaN"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>org</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12345.0</td>\n",
       "      <td>Sally</td>\n",
       "      <td>Some News Organization</td>\n",
       "      <td>Editor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54321.0</td>\n",
       "      <td>George</td>\n",
       "      <td>Some Other News Organization</td>\n",
       "      <td>Reporter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12345.0</td>\n",
       "      <td>Sally</td>\n",
       "      <td>Some News Organization</td>\n",
       "      <td>Editor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49382.0</td>\n",
       "      <td>Sally</td>\n",
       "      <td>Some News Organization</td>\n",
       "      <td>Editor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39331.0</td>\n",
       "      <td>Pat</td>\n",
       "      <td>Some Other Other News Organization</td>\n",
       "      <td>Producer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    name                                 org  position\n",
       "0  12345.0   Sally              Some News Organization    Editor\n",
       "1  54321.0  George        Some Other News Organization  Reporter\n",
       "2  12345.0   Sally              Some News Organization    Editor\n",
       "3  49382.0   Sally              Some News Organization    Editor\n",
       "4  39331.0     Pat  Some Other Other News Organization  Producer"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading\n",
    "\n",
    "This just scratches the surface of what you can do in pandas. Here are some other resources to check out:\n",
    "\n",
    "- [Pythonic Data Cleaning With NumPy and Pandas](https://realpython.com/python-data-cleaning-numpy-pandas/)\n",
    "- [pandas official list of tutorials](https://pandas.pydata.org/pandas-docs/stable/tutorials.html)\n",
    "- [Karrie Kehoe's guide to cleaning data in pandas](https://github.com/KarrieK/pandas_data_cleaning)\n",
    "- [Data cleaning with Python](https://www.dataquest.io/blog/data-cleaning-with-python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
