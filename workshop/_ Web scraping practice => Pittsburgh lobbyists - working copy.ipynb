{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping practice: Pittsburgh lobbyists\n",
    "\n",
    "We're going to scrape [a database of people registered to lobby the city of Pittsburgh](http://www.openbookpittsburgh.com/SearchLobbyists.aspx).\n",
    "\n",
    "First, let's import the packages we'll need: `csv`, `requests`, `bs4`, `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv, requests, BeautifulSoup from bs4, pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noodle around\n",
    "\n",
    "Navigate to the page we're going to scrape. We want everyone -- what happens when you hit \"search\" without entering any criteria into the form? It works! (This won't always be the case for databases like this.) As of July 28, 2018, the search showed 83 lobbyists.\n",
    "\n",
    "Notice, too, that the URL has changed from this:\n",
    "\n",
    "`http://www.openbookpittsburgh.com/SearchLobbyists.aspx`\n",
    "\n",
    "To this:\n",
    "\n",
    "`http://www.openbookpittsburgh.com/SearchLobbyists.aspx?&page=0&cat=LobbyistName&sort=ASC&num=10&click=1`\n",
    "\n",
    "After the `?` are the URL _parameters_, separated by `&`:\n",
    "- `page=0`\n",
    "- `cat=LobbyistName`\n",
    "- `sort=ASC`\n",
    "- `num=10`\n",
    "- `click=1`\n",
    "\n",
    "These are the instructions that get passed to the database after we click search: Return results under the category \"Lobbyist Name,\" show 10 lobbyists at a time, sort ascending, starting with page zero (the first page).\n",
    "\n",
    "(What happens if you _do_ put something in the search field? A new parameter is added to the URL: `lobbyist=`. But we want everything, so we can ignore this.)\n",
    "\n",
    "What happens if we tweak the URL and instruct the database to show us _100_ results at a time? Try it:\n",
    "\n",
    "[`http://www.openbookpittsburgh.com/SearchLobbyists.aspx?&page=0&cat=LobbyistName&sort=ASC&num=100&click=1`](`http://www.openbookpittsburgh.com/SearchLobbyists.aspx?&page=0&cat=LobbyistName&sort=ASC&num=100&click=1`)\n",
    "\n",
    "Now we have everyone in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save web page locally\n",
    "\n",
    "So we've got our target URL -- if we request that page, we get back some HTML containing all the data we'd like to scrape.\n",
    "\n",
    "When possible, it's good practice to save local copies of the pages that you're scraping. That way you don't have to rely on a stable internet connection as you work on your scraper, and you can avoid sending unneccessary traffic to the target's server.\n",
    "\n",
    "Let's do that now.\n",
    "\n",
    "First, set up a couple of variables:\n",
    "- The base URL\n",
    "- A [dictionary](../reference/Python%20data%20types%20and%20basic%20syntax.ipynb#Dictionaries) of URL parameters (see [the requests documentation here](http://docs.python-requests.org/en/master/user/quickstart/#passing-parameters-in-urls))\n",
    "- The name of the `.html` file we'll save to locally\n",
    "- The name of the `.csv` file we'll write out our results to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'http://www.openbookpittsburgh.com/SearchLobbyists.aspx'\n",
    "\n",
    "URL_PARAMS = {\n",
    "    'page': 0,\n",
    "    'cat': 'LobbyistName',\n",
    "    'sort': 'ASC',\n",
    "    'num': 1000,\n",
    "    'click': 1\n",
    "}\n",
    "\n",
    "HTML_FILE = 'pittsburgh-lobbyists.html'\n",
    "CSV_FILE = 'pittsburgh-lobbyists.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now actually fetch the page, specifying our headers and `params=URL_PARAMS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request the page\n",
    "# specify URL to get, custom headers and `params`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the `text` attribute -- the code underpinning the requested page -- to the file under the name we just specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open html file\n",
    "\n",
    "    # and write the page text into it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have a copy of the webpage in this directory. Let's open it up and turn the contents into a `BeautifulSoup` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the html file we just made\n",
    "\n",
    "    # read in the contents and turn them into soup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to start looking for patterns and isolating the HTML elements we want to target. I like to examine the source code in the browser (In Chrome, it's `Ctrl+U` on PCs and `Ctrl+option+U` on a Mac).\n",
    "\n",
    "It looks like all of the lobbyist HTML is enclosed in a `div` with the class `items-container`. Let's use the BeautifulSoup method `find` to isolate that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the container\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within that container, it looks like each individual entry is a `div` with the class `item`. Let's use `find_all` to return a list of matching elements within the container.\n",
    "\n",
    "Then we can use the built-in [`len()`](https://docs.python.org/3/library/functions.html#len) function to see how many we've got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the length of the list of items with len()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good! Let's grab _one_ of those items as a test and parse out the information. We'll then use what we learned to scrape the info out of each entry, one at a time.\n",
    "\n",
    "Lobbyists have multiple clients, so in our database, one record will be one lobbying relationship -- each line is, essentially, a client and the lobbyist representing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the first item and call it `test`\n",
    "\n",
    "\n",
    "# the person's name is in an h2 headline\n",
    "\n",
    "\n",
    "# their position is in a span element with the class `position`\n",
    "\n",
    "\n",
    "# their status is in two span elements that have the class `position`\n",
    "# the first is currency (expired or current)\n",
    "# the second is, are they a lobbyist for the city?\n",
    "# find_all() returns a list\n",
    "\n",
    "\n",
    "# grab text of \"currency\" status tag\n",
    "\n",
    "\n",
    "# set a default value -- they're assumed to not be a city lobbyist\n",
    "\n",
    "\n",
    "# unless the word \"yes\" appears in the (lowercased) city lobbyist span text\n",
    "# in which case, flip that variable to true\n",
    "\n",
    "\n",
    "\n",
    "# the company is in a div with the class `type`\n",
    "\n",
    "\n",
    "# the company address is in a div with the class `title`\n",
    "\n",
    "\n",
    "# lobbyists can have one or more clients, and these are list items in an unordered list\n",
    "# use find_all() to get all of the list items\n",
    "\n",
    "\n",
    "# loop over the list of clients\n",
    "\n",
    "    \n",
    "    # the company is in a span with the class `company`\n",
    "    # we'll also strip off the colon at the end and kill any external whitespace\n",
    "    # https://www.tutorialspoint.com/python/string_rstrip.htm\n",
    "\n",
    "    \n",
    "    # the company address is in a span with the class `address`\n",
    "\n",
    "\n",
    "    # use a trick to strip out internal whitespace\n",
    "    # https://stackoverflow.com/a/3739939\n",
    "\n",
    "    \n",
    "    # print the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solid. Now we can basically copy-paste that code into a [for loop](../reference/Python%20data%20types%20and%20basic%20syntax.ipynb#for-loops) and apply it to each item we found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every item in our list\n",
    "\n",
    "    # name is in the h2 tag -- text.strip()\n",
    "\n",
    "    # position is span with class position\n",
    "\n",
    "    # status is split across two span tags with a class called 'status'\n",
    "    # use find_all() to get them in a list\n",
    "\n",
    "    \n",
    "    # first one [0] is status\n",
    "\n",
    "\n",
    "    # default value -- they're not a lobbyist for the city\n",
    "\n",
    "    # if 'yes' is in the lowercase text of the second 'status' span\n",
    "\n",
    "        # then they _are_ a city lobbyist\n",
    "\n",
    "    # company is a div with class `type`\n",
    "\n",
    "    # company address is a div with class `title`\n",
    "\n",
    "    # clients are a bunch of list items -- use find_all() to get them\n",
    "\n",
    "    # loop over client list\n",
    "\n",
    "        # client company is span with class `company`\n",
    "        # rstrip the colon and strip whitespace\n",
    "\n",
    "        # client address is span with class address\n",
    "\n",
    "        # remove internal whitespace\n",
    "\n",
    "        # print the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good! Now let's write everything out to a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the CSV_FILE in write mode, newline=''\n",
    "\n",
    "    # define headers\n",
    "    headers = ['name', 'position', 'status', 'city_lobbyist', 'company',\n",
    "               'company_address', 'client_company', 'client_address']\n",
    "\n",
    "    # create a DictWriter object\n",
    "\n",
    "    # write the headers\n",
    "\n",
    "    # loop over the items in our list\n",
    "\n",
    "        # name is h2\n",
    "\n",
    "        # position is span with class position\n",
    "\n",
    "        # statuses arae in spans with class status -- use find_all()\n",
    "\n",
    "        # first thing in that list is status\n",
    "\n",
    "        # assume not a lobbyist for the city\n",
    "\n",
    "        # but if 'yes' in text of second status tag\n",
    "\n",
    "            # flip to True\n",
    "\n",
    "        # company info is div with class `type`\n",
    "\n",
    "        # company address is div with class `title`\n",
    "\n",
    "        # find_all() to get list of `li` tags of clients\n",
    "\n",
    "        # loop over client list\n",
    "            # client company in span with class `company`\n",
    "            # rstrip() colon and strip() whitespace\n",
    "\n",
    "            # client address is span with class `address`\n",
    "\n",
    "            # remove external whitespace\n",
    "\n",
    "            # write out to file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Extra credit_\n",
    "\n",
    "We're repeating ourselves a lot here. If I were going to publish this scraper, I'd probably clean this up into a series of functions that each do one thing. Some homework, if you're interested: Break down the processing we've done into major tasks (fetch the page, save to file, parse the contents) and write [functions](../reference/Functions.ipynb) for each task.\n",
    "\n",
    "(Eventually, as you progress in your coding journey, [this handy guide to refactoring](https://refactoring-101.readthedocs.io/en/latest/) will become very useful!.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into pandas for analysis\n",
    "\n",
    "Congrats! You've scraped a web page into a clean CSV. Here's where you could load it up into pandas and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in our csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use head() to check it out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what else?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
